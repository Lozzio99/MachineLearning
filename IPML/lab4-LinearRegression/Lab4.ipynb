{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8bQyBOaj8pUp"
   },
   "source": [
    "# Assignment: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GibImbUt9M4X"
   },
   "source": [
    "## Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BJqENzQL6_h7",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dh6VL5uH9TB8"
   },
   "source": [
    "## Function to calculate the Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3TDaV-br9agU",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, t, alpha, m, max_steps):\n",
    "    thetaHist = np.empty([max_steps,2])\n",
    "    for i in range(0,max_steps):\n",
    "        cost = cost_function(x,y,t)\n",
    "        t = t- (1/m) * alpha * cost\n",
    "        thetaHist[i] = t\n",
    "    return t, thetaHist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uuU6OmGcAEFn"
   },
   "source": [
    "## Function to calculate the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqXN74O2AHw3",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def cost_function(x, y, theta):\n",
    "    # HERE YOU HAVE TO IMPLEMENT THE COST FUNCTION\n",
    "    t_transpose = np.transpose(theta)\n",
    "    ht = np.dot(x,t_transpose)\n",
    "    cost = 0\n",
    "    for i in range(len(x)):\n",
    "        cost += (ht[i]-y[i]) ** 2\n",
    "\n",
    "    return cost / 2 * len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjmlKvgS9fnl"
   },
   "source": [
    "## Define some training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3TVU7If9jfU",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    x = np.array([[1, 0], [1, 0.5], [1, 1], [1, 1.5], [1, 2], [1, 2.5], [1, 3], [1, 4], [1, 5]])\n",
    "    y = np.array([0, 0.5, 1, 1.5, 2, 2.5, 3, 4, 5])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def init_coefficients(a1, b1, a2, b2):\n",
    "    x = b1 + a1 * np.random.rand(100, )\n",
    "    x_ = np.c_[np.ones((100, 1)), x]  # add x0 = 1 to each instance\n",
    "    y = b2 + a2 * x + np.random.rand(100, )\n",
    "    return x_, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHEXm5GT9118"
   },
   "source": [
    "## Cost function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qQTbR1G-BQ9",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_cost_function(x, y, theta0, theta1, m, J):\n",
    "\n",
    "    for i in range(0, len(theta0)):\n",
    "        for j in range(0, len(theta1)):\n",
    "            c = cost_function(x, y, [theta0[i], theta1[j]])\n",
    "            J[i, j] = c\n",
    "\n",
    "    theta0, theta1 = np.meshgrid(theta0, theta1)\n",
    "    fig2 = plt.figure(2)\n",
    "    ax = fig2.add_subplot(121, projection=\"3d\")\n",
    "    ax.plot_surface(theta0, theta1, np.transpose(J))\n",
    "    ax.set_xlabel('theta 0')\n",
    "    ax.set_ylabel('theta 1')\n",
    "    ax.set_zlabel('Cost J')\n",
    "    ax.set_title('Cost function Surface plot')\n",
    "    ax = fig2.add_subplot(122)\n",
    "    ax.contour(theta0, theta1, np.transpose(J))\n",
    "    ax.set_xlabel('theta 0')\n",
    "    ax.set_ylabel('theta 1')\n",
    "    ax.set_title('Cost function Contour plot')\n",
    "    fig2.subplots_adjust(bottom=0.1, right=1.5, top=0.9)\n",
    "    plt.show()\n",
    "    return J, theta0, theta1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p4FGqgkt-XjE"
   },
   "source": [
    "## Gradient descent implementation\n",
    "Here we implement Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TGofDHGV-dRk",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_hypothesis_function(th, x, y, alpha, m, max_steps, theta0, theta1, J, hist):\n",
    "    t, thetaHist = gradient_descent(x, y, th, alpha, m, max_steps)\n",
    "    if hist:\n",
    "        plt.figure(3)\n",
    "        plt.contour(theta0, theta1, np.transpose(J))\n",
    "        plt.plot(thetaHist[:, 0], thetaHist[:, 1], 'x')\n",
    "        plt.show()\n",
    "    xs = np.array([x.min(), x.max()])\n",
    "    h = np.array([[t[1] * xs[0] + t[0]], [t[1] * xs[1] + t[0]]])\n",
    "    plt.figure(1)\n",
    "    plt.plot(x[:, 1], y, 'x')  # Data\n",
    "    plt.plot(xs, h, '-o')  # hypothesis function\n",
    "    plt.show()\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXmsqwP--v5k"
   },
   "source": [
    "## Testing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 566,
     "status": "error",
     "timestamp": 1594188267487,
     "user": {
      "displayName": "Enrique Hortal Quesada",
      "photoUrl": "",
      "userId": "12024213593623682680"
     },
     "user_tz": -120
    },
    "id": "7HAhC2Kw-0w0",
    "outputId": "d7bef4a7-31cf-4697-d850-a8e8f439859a",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def test(th, x, y, alpha, max_steps, data, cost):\n",
    "    theta0 = np.arange(-2, 2.01, 0.25)\n",
    "    theta1 = np.arange(-2, 3.01, 0.25)\n",
    "    J = np.zeros((len(theta0), len(theta1)))\n",
    "    m, n = np.shape(x)\n",
    "    if data:\n",
    "        plt.figure(1)  # An empty figure with no axes\n",
    "        plt.plot(x[:, 1], y, 'x')\n",
    "        plt.show()\n",
    "    if cost:\n",
    "        J, theta0, theta1 = plot_cost_function(x, y, theta0, theta1, m, J)\n",
    "        plot_hypothesis_function(th, x, y, alpha, m, max_steps, theta0, theta1, J, True)\n",
    "    else :\n",
    "        plot_hypothesis_function(th, x, y, alpha, m, max_steps, theta0, theta1, J, False)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing with given parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, Y = init()\n",
    "thetaN = [2, 0]\n",
    "a = 0.05  # learning parameter\n",
    "m_steps = 1000  # number of iterations that the algorithm is running\n",
    "test(thetaN, X, Y, a, m_steps, True, True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing with low alpha\n",
    "As we can see from the hypothesis function a low alpha value will not get even close to the expected predictions,\n",
    "indeed they are still quite close to the given initial values [2,0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = 1e-8  # learning parameter\n",
    "m_steps = 1000  # number of iterations that the algorithm is running\n",
    "test(thetaN, X, Y, a, m_steps, False, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing with high alpha\n",
    "An alpha value too high will not make the method converge because it will tilt\n",
    "around too quick without balancing its slope and rise.\n",
    "Also, the predicted values are quite far from the data points, meaning the method failed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = 1  # learning parameter\n",
    "m_steps = 1000  # number of iterations that the algorithm is running\n",
    "test(thetaN, X, Y , a, m_steps, False, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finding absolute global minimum\n",
    "**Can Linear Regression really find the absolute global minimum?**\n",
    "There is no guarantee that the method can find the absolute global minimum without exceeding overfitting.\n",
    "The reason for this is also the maximum amount of iterations taken and the intent of minimizing the loss\n",
    "function.\n",
    "However, with variations of the heuristic one can eventually escape local minima in search of the global one.\n",
    "\n",
    "\n",
    "## Changing initial theta prediction\n",
    "As we can see, with a good alpha value and enough iterations, the method will also eventually converge to the\n",
    "correct theta values, and as we can see from the theta history the convergence is quite fast, too."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "thetaN = [-200, 100]\n",
    "a = 0.05  # learning parameter\n",
    "m_steps = 1000  # number of iterations that the algorithm is running\n",
    "test(thetaN, X, Y, a, m_steps, True, True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Updating theta0 and theta1 separately\n",
    "All theta values needs to be updated together at the same time in order to \"move\"\n",
    "the function line towards the right direction.\n",
    "As we know the prediction of the trained model will return a linear function between x and theta,\n",
    "as following : $$t_1 * x_i + t_0$$ , for every $x_i$  in $x$\n",
    "\n",
    "\n",
    "\n",
    "## How many iterations to compute exact theta\n",
    "To get exact theta values from the method a huge number of iterations with the most suitable alpha is needed\n",
    "\n",
    "## Testing with noise\n",
    "As we can see the method performs good with noised data and random initial theta"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, Y = init_coefficients(2,1,-2,-3)\n",
    "thetaN = np.random.rand(2,)\n",
    "a = 0.05  # learning parameter\n",
    "m_steps = 1000  # number of iterations that the algorithm is running\n",
    "test(thetaN, X, Y, a, m_steps, True, True)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMj3lnx6l1he2CUNg1xIsQJ",
   "collapsed_sections": [],
   "name": "LinearRegression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}