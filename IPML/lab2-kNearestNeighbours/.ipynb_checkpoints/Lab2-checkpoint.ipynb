{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import average\n",
    "from pandas import unique\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "class kNN:\n",
    "    def __init__(self, k=3, exp=2):\n",
    "        # constructor for kNN classifier\n",
    "        # k is the number of neighbor for local class estimation\n",
    "        # exp is the exponent for the Minkowski distance\n",
    "        self.s = 1e-6\n",
    "        self.k = k\n",
    "        self.exp = exp\n",
    "\n",
    "    def fit(self, xtr, ytr):\n",
    "        # training k-NN method X_train is the training data given with input attributes. n-th row correponds to n-th\n",
    "        # instance. Y_train is the output data (output vector): n-th element of Y_train is the output value for n-th\n",
    "        # instance in X_train.\n",
    "        self.X_train = xtr\n",
    "        self.Y_train = ytr\n",
    "\n",
    "    def get_discrete_classification(self, X_test):\n",
    "        # predict-class k-NN method\n",
    "        # X_test is the test data given with input attributes. Rows correspond to instances\n",
    "        # Method outputs prediction vector Y_pred_test:  n-th element of Y_pred_test is the prediction\n",
    "        # for n-th instance in X_test\n",
    "\n",
    "        Y_pred_test = []  # prediction vector Y_pred_test for all the test instances\n",
    "        # in X_test is initialized to empty list []\n",
    "\n",
    "        for i in range(len(X_test)):  # iterate over all instances in X_test\n",
    "            test_instance = X_test.iloc[i]  # i-th test instance\n",
    "\n",
    "            distances = []  # list of distances of the i-th test_instance for all the\n",
    "            # train_instance s in X_train, initially empty.\n",
    "\n",
    "            for j in range(len(self.X_train)):  # iterate over all instances in X_train\n",
    "                train_instance = self.X_train.iloc[j]  # j-th training instance\n",
    "                distance = self.minkowski_distance(test_instance,\n",
    "                                                   train_instance)  # distance between i-th test instance and j-th\n",
    "                # training instance\n",
    "                distances.append(distance)  # add the distance to the list of distances of the i-th test_instance\n",
    "\n",
    "            # Store distances in a dataframe. The dataframe has the index of Y_train in order to keep the\n",
    "            # correspondence with the classes of the training instances\n",
    "            df_dists = pd.DataFrame(data=distances, columns=['dist'], index=self.Y_train.index)\n",
    "\n",
    "            # Sort distances, and only consider the k closest points in the new dataframe df_knn\n",
    "            df_nn = df_dists.sort_values(by=['dist'], axis=0)\n",
    "            df_knn = df_nn[:self.k]\n",
    "\n",
    "            # Note that the index df_knn.index of df_knn contains indices in Y_train of the k-closed training\n",
    "            # instances to the i-th test instance. Thus, the dataframe self.Y_train[df_knn.index] contains the\n",
    "            # classes of those k-closed training instances. Method value_counts() computes the counts (number of\n",
    "            # occurrences) for each class in self.Y_train[df_knn.index] in dataframe predictions.\n",
    "            predictions = self.Y_train[df_knn.index].value_counts()\n",
    "\n",
    "            # the first element of the index predictions.index contains the class with the highest count; i.e. the\n",
    "            # prediction y_pred_test.\n",
    "            y_pred_test = predictions.index[0]\n",
    "            # add the prediction y_pred_test to the prediction vector Y_pred_test for all the test instances in X_test\n",
    "            Y_pred_test.append(y_pred_test)\n",
    "\n",
    "        return Y_pred_test\n",
    "\n",
    "    def minkowski_distance(self, x1, x2):\n",
    "        # computes the Minkowski distance of x1 and x2 for two labeled instances (x1,y1) and (x2,y2)\n",
    "\n",
    "        # Set initial distance to 0\n",
    "        distance = 0\n",
    "\n",
    "        # Calculate Minkowski distance using the exponent exp\n",
    "        for i in range(len(x1)):\n",
    "            distance = distance + abs(x1[i] - x2[i]) ** self.exp\n",
    "\n",
    "        distance = distance ** (1 / self.exp)\n",
    "\n",
    "        return distance\n",
    "\n",
    "    def get_class_probs(self, X_test):\n",
    "        # function to evaluate posterior classes probabilities\n",
    "        classes = unique(self.Y_train)  # set of unique classes\n",
    "        # create dataframe with columns = prob(class_i) and index = test_instances.index\n",
    "        probs = pd.DataFrame(data=np.zeros((len(X_test.index), len(classes))),\n",
    "                             columns=classes,\n",
    "                             index=X_test.index)\n",
    "\n",
    "        for i in range(len(X_test)):  # for each test instance\n",
    "            predictions = self.predict(X_test, i)\n",
    "            # calculate probability for each class\n",
    "            for x in predictions.index:\n",
    "                instance = probs.iloc[i]\n",
    "                instance.loc[x] = self.get_class_probability(predictions[x], len(classes))\n",
    "\n",
    "        # return join of the two tables\n",
    "        return pd.concat([X_test, probs], axis=1)\n",
    "\n",
    "    def get_class_probability(self, n_instances_class_i, n_classes):\n",
    "        # function to evaluate the probability for an instance to belong to a certain class\n",
    "        # given #(instances of class i) and total #classes\n",
    "        return (n_instances_class_i + self.s) / (self.k + n_classes * self.s)\n",
    "\n",
    "    def get_prediction(self, X_test):\n",
    "        # function to evaluate regression value for the output attribute\n",
    "        vals = pd.DataFrame(data=np.zeros((len(X_test.index), 1)),\n",
    "                            columns=['regression value'],\n",
    "                            index=X_test.index)\n",
    "        for i in range(len(X_test)):\n",
    "            predictions = self.predict(X_test, i)\n",
    "            vals.iloc[i] = average(predictions.index)\n",
    "            # allocate at index of the test instance the average value of the k predictions\n",
    "        return pd.concat([X_test, vals], axis=1)\n",
    "\n",
    "    def predict(self, X_test, i):\n",
    "        distances = []\n",
    "        for j in range(len(self.X_train)):  # find neighbours\n",
    "            distance = self.minkowski_distance(X_test.iloc[i], self.X_train.iloc[j])\n",
    "            distances.append(distance)\n",
    "\n",
    "        df_dists = pd.DataFrame(data=distances, columns=['dist'], index=self.Y_train.index)\n",
    "        df_knn = df_dists.sort_values(by=['dist'], axis=0)[:self.k]\n",
    "        return self.Y_train[df_knn.index].value_counts()  # select and return the k-nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Data preparation: Normalizing dataset\n",
    "##################################################\n",
    "def normalize(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Data read: Training and Test set creation\n",
    "##################################################\n",
    "def read_data(file_name):\n",
    "    data = pd.read_csv(file_name)\n",
    "    y = data['class']\n",
    "    x = data.drop(['class'], axis=1)\n",
    "    data.head()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.34, random_state=10)\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Hold-out testing: Variation on k parameter\n",
    "##################################################\n",
    "def hold_out_k_range(xtr, xt, ytr, yt, k_r, title):\n",
    "    tr_acc = np.zeros(len(k_r))\n",
    "    t_acc = np.zeros(len(k_r))\n",
    "    i = 0\n",
    "    for k in k_r:\n",
    "        clf = kNN(k)\n",
    "        clf.fit(xtr, ytr)\n",
    "        yp_tr = clf.get_discrete_classification(xtr)\n",
    "        yp_t = clf.get_discrete_classification(xt)\n",
    "        tr_acc[i] = accuracy_score(ytr, yp_tr)\n",
    "        t_acc[i] = accuracy_score(yt, yp_t)\n",
    "        i += 1\n",
    "\n",
    "    plt.plot(k_r, tr_acc, 'ro-', k_r, t_acc, 'bv--')\n",
    "    plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Hold-out testing: Variation on exp parameter\n",
    "##################################################\n",
    "def hold_out_exp_range(xtr, xt, ytr, yt, exp_r, title):\n",
    "    tr_acc = np.zeros(len(exp_r))\n",
    "    t_acc = np.zeros(len(exp_r))\n",
    "    i = 0\n",
    "    for exp in exp_r:\n",
    "        clf = kNN(k=3, exp=exp)\n",
    "        clf.fit(xtr, ytr)\n",
    "        yp_tr = clf.get_discrete_classification(xtr)\n",
    "        yp_t = clf.get_discrete_classification(xt)\n",
    "        tr_acc[i] = accuracy_score(ytr, yp_tr)\n",
    "        t_acc[i] = accuracy_score(yt, yp_t)\n",
    "        i += 1\n",
    "\n",
    "    plt.plot(exp_r, tr_acc, 'ro-', exp_r, t_acc, 'bv--')\n",
    "    plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
    "    plt.xlabel('exp')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Hold-out testing: Variation on k parameter\n",
    "##################################################\n",
    "def hold_out_regression_value_k_range(xtr, xt, ytr, yt, k_r, title):\n",
    "    tr_acc = np.zeros(len(k_r))\n",
    "    t_acc = np.zeros(len(k_r))\n",
    "    i = 0\n",
    "    for k in k_r:\n",
    "        clf = kNN(k)\n",
    "        clf.fit(xtr, ytr)\n",
    "        yp_tr = clf.get_prediction(xtr)['regression value']\n",
    "        yp_t = clf.get_prediction(xt)['regression value']\n",
    "        tr_acc[i] = mean_abs_err(ytr.values, yp_tr)\n",
    "        t_acc[i] = mean_abs_err(yt.values, yp_t)\n",
    "        i += 1\n",
    "    #########################################\n",
    "    # Plot of training and test accuracies\n",
    "    #########################################\n",
    "    plt.plot(k_r, tr_acc, 'ro-', k_r, t_acc, 'bv--')\n",
    "    plt.legend(['Training Error', 'Test Error'])\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mean_abs_err(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_testing_k_range(x_train, x_test, y_train, y_test, k_r, s):\n",
    "    # function to test and plot accuracy for hold-out validation\n",
    "    # with non-normalized and with normalized data\n",
    "    # in function of the k parameter of the kNN\n",
    "    hold_out_k_range(x_train, x_test, y_train, y_test, k_r, s + ' non-normalized k range')\n",
    "    hold_out_k_range(normalize(x_train), normalize(x_test), y_train, y_test, k_r, s + ' normalized k range')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def accuracy_testing_exp_range(x_train, x_test, y_train, y_test, exp_r, s):\n",
    "    # function to test and plot accuracy for hold-out validation\n",
    "    # with non-normalized and with normalized data\n",
    "    # in function of the exp parameter of the kNN\n",
    "    hold_out_exp_range(x_train, x_test, y_train, y_test, exp_r, s + ' non-normalized exp range')\n",
    "    hold_out_exp_range(normalize(x_train), normalize(x_test), y_train, y_test, exp_r, s + ' normalized exp range')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Test  the kNN classifier  on  the diabetes and glass classification  data  sets for the case when the data\n",
    "is not normalized and the case when the data is normalized in function of parameter k of the kNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k_range = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]\n",
    "exp_range = [2, 100, 1000, 10000]\n",
    "\n",
    "diabetes = 'data/diabetes.csv'\n",
    "glass = 'data/glass.csv'\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = read_data(diabetes)\n",
    "accuracy_testing_k_range(X_Train, X_Test, Y_Train, Y_Test, k_range, 'diabetes')\n",
    "\n",
    "X_Train, X_Test, Y_Train, Y_Test = read_data(glass)\n",
    "accuracy_testing_k_range(X_Train, X_Test, Y_Train, Y_Test, k_range, 'glass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**_Indicate whether the training and hold-out accuracy rates improve with normalization._**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
