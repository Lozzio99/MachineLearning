{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class of k-Nearest Neighbour Classifier\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "class kNN:\n",
    "    def __init__(self, k = 3, exp = 2):\n",
    "    # constructor for kNN classifier \n",
    "    # k is the number of neighbor for local class estimation\n",
    "    # exp is the exponent for the Minkowski distance\n",
    "        self.k = k\n",
    "        self.exp = exp\n",
    "    def fit(self, X_train, Y_train):\n",
    "    # training k-NN method\n",
    "    # X_train is the training data given with input attributes. n-th row correponds to n-th instance.\n",
    "    # Y_train is the output data (output vector): n-th element of Y_train is the output value for n-th instance in X_train.\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "    def get_discrete_classification(self, X_test):\n",
    "    # predict-class k-NN method\n",
    "    # X_test is the test data given with input attributes. Rows correpond to instances\n",
    "    # Method outputs prediction vector Y_pred_test:  n-th element of Y_pred_test is the prediction for n-th instance in X_test\n",
    "    \n",
    "        Y_pred_test = [] #prediction vector Y_pred_test for all the test instances in X_test is initialized to empty list []\n",
    "\n",
    "   \n",
    "        for i in range(len(X_test)):   #iterate over all instances in X_test\n",
    "            test_instance = X_test.iloc[i] #i-th test instance \n",
    "            \n",
    "            distances = []  #list of distances of the i-th test_instance for all the train_instance s in X_train, initially empty.\n",
    "          \n",
    "            for j in range(len(self.X_train)):  #iterate over all instances in X_train\n",
    "                train_instance = self.X_train.iloc[j] #j-th training instance \n",
    "                distance = self.minkowski_distance(test_instance, train_instance) #distance between i-th test instance and j-th training instance\n",
    "                distances.append(distance) #add the distance to the list of distances of the i-th test_instance\n",
    "        \n",
    "            # Store distances in a dataframe. The dataframe has the index of Y_train in order to keep the correspondence with the classes of the training instances \n",
    "            df_dists = pd.DataFrame(data=distances, columns=['dist'], index = self.Y_train.index)\n",
    "        \n",
    "            # Sort distances, and only consider the k closest points in the new dataframe df_knn\n",
    "            df_nn = df_dists.sort_values(by=['dist'], axis=0)\n",
    "            df_knn =  df_nn[:self.k]\n",
    "            \n",
    "            # Note that the index df_knn.index of df_knn contains indices in Y_train of the k-closed training instances to \n",
    "            # the i-th test instance. Thus, the dataframe self.Y_train[df_knn.index] contains the classes of those k-closed \n",
    "            # training instances. Method value_counts() computes the counts (number of occurencies) for each class in \n",
    "            # self.Y_train[df_knn.index] in dataframe predictions. \n",
    "            predictions = self.Y_train[df_knn.index].value_counts()\n",
    "                 \n",
    "            # the first element of the index predictions.index contains the class with the highest count; i.e. the prediction y_pred_test.\n",
    "            y_pred_test = predictions.index[0]\n",
    "\n",
    "            # add the prediction y_pred_test to the prediction vector Y_pred_test for all the test instances in X_test\n",
    "            Y_pred_test.append(y_pred_test)\n",
    "        \n",
    "        return Y_pred_test\n",
    "    def minkowski_distance(self, x1, x2):\n",
    "    # computes the Minkowski distance of x1 and x2 for two labeled instances (x1,y1) and (x2,y2)\n",
    "    \n",
    "        # Set initial distance to 0\n",
    "        distance = 0\n",
    "    \n",
    "        # Calculate Minkowski distance using the exponent exp\n",
    "        for i in range(len(x1)):\n",
    "            distance = distance + abs(x1[i] - x2[i])**self.exp\n",
    "        \n",
    "        distance = distance**(1/self.exp)\n",
    "    \n",
    "        return distance\n",
    "    @staticmethod\n",
    "    def normalize(df):\n",
    "        column_maxes = df.max()\n",
    "        df_max = column_maxes.max()\n",
    "        column_mins = df.min()  # if dataset contains negative values\n",
    "        df_min = column_mins.min()\n",
    "        return (df - df_min) / (df_max - df_min)\n",
    "    @staticmethod\n",
    "    def read_data(file_name):\n",
    "        data = pd.read_csv(file_name)\n",
    "        y = data['class']\n",
    "        x = data.drop(['class'],axis = 1)\n",
    "        xtr, xt, ytr, yt = train_test_split(x,y,test_size=0.34, random_state=10)\n",
    "        return  xtr, xt, ytr, yt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Hold-out testing: Training and Test set creation\n",
    "##################################################\n",
    "def hold_out_k_range(filename):\n",
    "    xtr, xt, ytr, yt = kNN.read_data(filename)\n",
    "    # range for the values of parameter k for kNN\n",
    "    k_range = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31]\n",
    "\n",
    "    tr_acc = np.zeros(len(k_range))\n",
    "    t_acc = np.zeros(len(k_range))\n",
    "\n",
    "    i = 0\n",
    "    for k  in  k_range:\n",
    "        clf = kNN(k)\n",
    "        clf.fit(xtr, ytr)\n",
    "        yp_tr = clf.get_discrete_classification(xtr)\n",
    "        yp_t = clf.get_discrete_classification(xt)\n",
    "        tr_acc[i] = accuracy_score(ytr, yp_tr)\n",
    "        t_acc[i] = accuracy_score(yt, yp_t)\n",
    "        i += 1\n",
    "\n",
    "    #########################################\n",
    "    # Plot of training and test accuracies\n",
    "    #########################################\n",
    "    plt.plot(k_range,tr_acc,'ro-',k_range,t_acc,'bv--')\n",
    "    plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Hold-out testing: Training and Test set creation\n",
    "##################################################\n",
    "def hold_out_exp_range(filename):\n",
    "    xtr, xt, ytr, yt = kNN.read_data(filename)\n",
    "    # range for the values of parameter exp for kNN\n",
    "\n",
    "    exp_range = [2,  100, 10000]\n",
    "\n",
    "    tr_acc = np.zeros(len(exp_range))\n",
    "    t_acc = np.zeros(len(exp_range))\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    for exp  in  exp_range:\n",
    "        clf = kNN(k = 3, exp = exp)\n",
    "        clf.fit(xtr, ytr)\n",
    "        yp_tr = clf.get_discrete_classification(xtr)\n",
    "        yp_t = clf.get_discrete_classification(xt)\n",
    "        tr_acc[i] = accuracy_score(ytr, yp_tr)\n",
    "        tr_acc[i] = accuracy_score(yt, yp_t)\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    #########################################\n",
    "    # Plot of training and test accuracies\n",
    "    #########################################\n",
    "\n",
    "    plt.plot(exp_range,tr_acc,'ro-',exp_range,t_acc,'bv--')\n",
    "    plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "    plt.xlabel('exp')\n",
    "    plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_err():\n",
    "    y_true = [3, -0.5, 2, 7]\n",
    "    y_pred = [2.5, 0.0, 2, 8]\n",
    "    return mean_absolute_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 2\n",
    "## task B\n",
    "**Test the kNN classifier on the diabetes and glass classification data sets for the case when the data\n",
    "is not normalized and the case when the data is normalized.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = 'data/diabetes.csv'\n",
    "hold_out_k_range(filename= filename)\n",
    "hold_out_exp_range(filename=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Indicate whether the training and hold-out accuracy rates improve with normalization.**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
